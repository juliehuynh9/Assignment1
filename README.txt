Assignment 1: Text File Tokenizer

This project is part of an assignment for the CS121 course. It involves creating a text file tokenizer that reads in a text file and returns a list of tokens. A token is defined as a sequence of alphanumeric characters, independent of capitalization.


Functionality:

PartA:
Tokenizer Function: The tokenize function in PartA.py reads in a text file byte by byte and returns a list of tokens. It handles cases where tokens span multiple lines and handles newline characters.
Computing Word Frequencies: It will count the number of occurrences a token shows up in the text file.
Printing Tokens and Frequencies: Based on the 2 previous functions, it will print out the results.

PartB:
Common Words Finder: The find_common_words function in PartB.py identifies common words between two text files by utilizing the tokenizer function.

Installation:
To use this project, follow these steps:

Clone the repository to your local machine: git clone https://github.com/juliehuynh9/Assignment1.git
Navigate to the project directory: cd Assignment1

***MAKE SURE TO HAVE SOME TEST FILES READY***
Add any text files in the project folder directly so it's easier to add in the parameters without listing all the directories

Usage:

For PartA and PartB, to use it you will have to use the terminal.
In order to test it out, you need to write the python script for it.
(Ex. PS C:\CS121\Assignment_1> python PartA.py test.txt)
(Ex. PS C:\CS121\Assignment_1> python PartB.py test1.txt test2.txt)


Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.

License
This project is licensed under the MIT License. See the LICENSE file for details.